{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "587c81fb",
   "metadata": {},
   "source": [
    "## Purpose:\n",
    "- this notebook illustrates how preprocessing is done for question answering tasks\n",
    "- source: https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb9d279",
   "metadata": {},
   "source": [
    "### 1. Declare initial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3f56f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This flag is the difference between SQUAD v1 or 2 (if you're using another dataset, it indicates if impossible\n",
    "# answers are allowed or not).\n",
    "squad_v2 = False\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53999a75",
   "metadata": {},
   "source": [
    "### 2. Load dataset\n",
    "- in this case squad v1 is used - does not have \"no answers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf8fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b07021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\tanch\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41)\n"
     ]
    }
   ],
   "source": [
    "# load squad dataset\n",
    "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdc48752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dataset has already been split to training and validation sets\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9035d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'id': '5733be284776f41900661182',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'title': 'University_of_Notre_Dame'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each row is retrieved as a single dictionary\n",
    "datasets[\"train\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac02fb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': [{'answer_start': [188], 'text': ['a copper statue of Christ']},\n",
       "  {'answer_start': [279], 'text': ['the Main Building']},\n",
       "  {'answer_start': [381],\n",
       "   'text': ['a Marian place of prayer and reflection']}],\n",
       " 'context': ['Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       "  'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       "  'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'],\n",
       " 'id': ['5733be284776f4190066117f',\n",
       "  '5733be284776f41900661180',\n",
       "  '5733be284776f41900661181'],\n",
       " 'question': ['What is in front of the Notre Dame Main Building?',\n",
       "  'The Basilica of the Sacred heart at Notre Dame is beside to which structure?',\n",
       "  'What is the Grotto at Notre Dame?'],\n",
       " 'title': ['University_of_Notre_Dame',\n",
       "  'University_of_Notre_Dame',\n",
       "  'University_of_Notre_Dame']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple rows are also retrieved as a single dictionary\n",
    "datasets[\"train\"][1,2,3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc44c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "def show_random_elements(dataset, num_examples = 10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be187235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answers</th>\n",
       "      <th>context</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'answer_start': [490], 'text': ['Brazil']}</td>\n",
       "      <td>They can also be armed with non-lethal (more accurately known as \"less than lethal\" or \"less-lethal\") weaponry, particularly for riot control. Non-lethal weapons include batons, tear gas, riot control agents, rubber bullets, riot shields, water cannons and electroshock weapons. Police officers often carry handcuffs to restrain suspects. The use of firearms or deadly force is typically a last resort only to be used when necessary to save human life, although some jurisdictions (such as Brazil) allow its use against fleeing felons and escaped convicts. A \"shoot-to-kill\" policy was recently introduced in South Africa, which allows police to use deadly force against any person who poses a significant threat to them or civilians. With the country having one of the highest rates of violent crime, president Jacob Zuma states that South Africa needs to handle crime differently from other countries.</td>\n",
       "      <td>5732bcead6dcfa19001e8a9c</td>\n",
       "      <td>Where can police shoot fleeing convicts?</td>\n",
       "      <td>Police</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'answer_start': [455], 'text': ['fix carbon from the air']}</td>\n",
       "      <td>A large percentage of herbivores have mutualistic gut flora that help them digest plant matter, which is more difficult to digest than animal prey. This gut flora is made up of cellulose-digesting protozoans or bacteria living in the herbivores' intestines. Coral reefs are the result of mutualisms between coral organisms and various types of algae that live inside them. Most land plants and land ecosystems rely on mutualisms between the plants, which fix carbon from the air, and mycorrhyzal fungi, which help in extracting water and minerals from the ground.</td>\n",
       "      <td>56de22074396321400ee25d3</td>\n",
       "      <td>How do plants contribute to terrestrial ecosystems?</td>\n",
       "      <td>Symbiosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'answer_start': [163], 'text': ['Fast Patrol Craft']}</td>\n",
       "      <td>During his tour on the guided missile frigate USS Gridley, Kerry requested duty in South Vietnam, listing as his first preference a position as the commander of a Fast Patrol Craft (PCF), also known as a \"Swift boat.\" These 50-foot (15 m) boats have aluminum hulls and have little or no armor, but are heavily armed and rely on speed. \"I didn't really want to get involved in the war\", Kerry said in a book of Vietnam reminiscences published in 1986. \"When I signed up for the swift boats, they had very little to do with the war. They were engaged in coastal patrolling and that's what I thought I was going to be doing.\" However, his second choice of billet was on a river patrol boat, or \"PBR\", which at the time was serving a more dangerous duty on the rivers of Vietnam.</td>\n",
       "      <td>572aa3c1111d821400f38c6a</td>\n",
       "      <td>What was the formal name of 'swift boats'?</td>\n",
       "      <td>John_Kerry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'answer_start': [47], 'text': ['Sunni branch']}</td>\n",
       "      <td>Christianity, Judaism, Zoroastrianism, and the Sunni branch of Islam are officially recognized by the government, and have reserved seats in the Iranian Parliament. But the Bahá'í Faith, which is said to be the largest non-Muslim religious minority in Iran, is not officially recognized, and has been persecuted during its existence in Iran since the 19th century. Since the 1979 Revolution, the persecution of Bahais has increased with executions, the denial of civil rights and liberties, and the denial of access to higher education and employment.</td>\n",
       "      <td>57303660947a6a140053d2a8</td>\n",
       "      <td>What other branch of Islam is recognized by the Iranian government?</td>\n",
       "      <td>Iran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'answer_start': [237], 'text': ['BBC Radio 1']}</td>\n",
       "      <td>Later in 2013, West launched a tirade on Twitter directed at talk show host Jimmy Kimmel after his ABC program Jimmy Kimmel Live! ran a sketch on September 25 involving two children re-enacting West's recent interview with Zane Lowe for BBC Radio 1 in which he calls himself the biggest rock star on the planet. Kimmel reveals the following night that West called him to demand an apology shortly before taping.</td>\n",
       "      <td>56d4672e2ccc5a1400d8314a</td>\n",
       "      <td>On what radio station did Kanye West deem himself \"the biggest rockstar on the planet\"?</td>\n",
       "      <td>Kanye_West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'answer_start': [711], 'text': ['St. John's United Methodist Church']}</td>\n",
       "      <td>Beyoncé attended St. Mary's Elementary School in Fredericksburg, Texas, where she enrolled in dance classes. Her singing talent was discovered when dance instructor Darlette Johnson began humming a song and she finished it, able to hit the high-pitched notes. Beyoncé's interest in music and performing continued after winning a school talent show at age seven, singing John Lennon's \"Imagine\" to beat 15/16-year-olds. In fall of 1990, Beyoncé enrolled in Parker Elementary School, a music magnet school in Houston, where she would perform with the school's choir. She also attended the High School for the Performing and Visual Arts and later Alief Elsik High School. Beyoncé was also a member of the choir at St. John's United Methodist Church as a soloist for two years.</td>\n",
       "      <td>56d443ef2ccc5a1400d830df</td>\n",
       "      <td>What choir did Beyoncé sing in for two years?</td>\n",
       "      <td>Beyoncé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'answer_start': [174], 'text': ['Dust and scratches']}</td>\n",
       "      <td>Vinyl records do not break easily, but the soft material is easily scratched. Vinyl readily acquires a static charge, attracting dust that is difficult to remove completely. Dust and scratches cause audio clicks and pops. In extreme cases, they can cause the needle to skip over a series of grooves, or worse yet, cause the needle to skip backwards, creating a \"locked groove\" that repeats over and over. This is the origin of the phrase \"like a broken record\" or \"like a scratched record\", which is often used to describe a person or thing that continually repeats itself. Locked grooves are not uncommon and were even heard occasionally in radio broadcasts.</td>\n",
       "      <td>5727e4fd2ca10214002d98eb</td>\n",
       "      <td>What is the cause of lock grooves on vinyl records?</td>\n",
       "      <td>Gramophone_record</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'answer_start': [304], 'text': ['Tartu']}</td>\n",
       "      <td>Estonia co-operates with Latvia and Lithuania in several trilateral Baltic defence co-operation initiatives, including Baltic Battalion (BALTBAT), Baltic Naval Squadron (BALTRON), Baltic Air Surveillance Network (BALTNET) and joint military educational institutions such as the Baltic Defence College in Tartu. Future co-operation will include sharing of national infrastructures for training purposes and specialisation of training areas (BALTTRAIN) and collective formation of battalion-sized contingents for use in the NATO rapid-response force. In January 2011 the Baltic states were invited to join NORDEFCO, the defence framework of the Nordic countries.</td>\n",
       "      <td>5728c1523acd2414000dfda9</td>\n",
       "      <td>Where is the Baltic Defence College located?</td>\n",
       "      <td>Estonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'answer_start': [310], 'text': ['Kathmandu International Theater Festival']}</td>\n",
       "      <td>Kathmandu is home to Nepali cinema and theaters. The city contains several theaters, including the National Dance Theatre in Kanti Path, the Ganga Theatre, the Himalayan Theatre and the Aarohan Theater Group founded in 1982. The M. Art Theater is based in the city. The Gurukul School of Theatre organizes the Kathmandu International Theater Festival, attracting artists from all over the world. A mini theater is also located at the Hanumandhoka Durbar Square, established by the Durbar Conservation and Promotion Committee.</td>\n",
       "      <td>5735c421dc94161900571ffd</td>\n",
       "      <td>What gathering is the work of the Gurukul School of Theatre?</td>\n",
       "      <td>Kathmandu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'answer_start': [3], 'text': ['1945']}</td>\n",
       "      <td>In 1945, the British entrepreneur J. Arthur Rank, hoping to expand his American presence, bought into a four-way merger with Universal, the independent company International Pictures, and producer Kenneth Young. The new combine, United World Pictures, was a failure and was dissolved within one year. Rank and International remained interested in Universal, however, culminating in the studio's reorganization as Universal-International. William Goetz, a founder of International, was made head of production at the renamed Universal-International Pictures Inc., which also served as an import-export subsidiary, and copyright holder for the production arm's films. Goetz, a son-in-law of Louis B. Mayer decided to bring \"prestige\" to the new company. He stopped the studio's low-budget production of B movies, serials and curtailed Universal's horror and \"Arabian Nights\" cycles. Distribution and copyright control remained under the name of Universal Pictures Company Inc.</td>\n",
       "      <td>56e161c3e3433e1400422e30</td>\n",
       "      <td>In what year was United World Pictures founded?</td>\n",
       "      <td>Universal_Studios</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visaulise the data in a table\n",
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86e51f6",
   "metadata": {},
   "source": [
    "### 3. Instantiate tokenizers:\n",
    "- this class splits words into sub words and into their corresponding IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "755bc9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the tokenzier \n",
    "# note that different models require different tokenizers\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb96cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the tokenizer we instantiated  is a fast tokenizer because we need its special features\n",
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c08ce",
   "metadata": {},
   "source": [
    "- these models have fast tokenizers\n",
    "    - https://huggingface.co/transformers/index.html#bigtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b03e8414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102, 2026, 2171, 2003, 25353, 22144, 2378, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see that we can tokenise a question ans answer pair\n",
    "# different tokenizers will give different tokens\n",
    "tokenized_output = tokenizer(\"What is your name?\", \"My name is Sylvain.\")\n",
    "tokenized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14a7863a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 25353, 22144, 2378, 102], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also tokenise one sentence\n",
    "# notice the word is split into multiple subwords\n",
    "tokenizer(\"Sylvain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9204bdf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] what is your name? [SEP] my name is sylvain. [SEP]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in decoding, we see that special tokens were automatically added - though we can specify otherwise\n",
    "tokenizer.decode(tokenized_output['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47729e78",
   "metadata": {},
   "source": [
    "### 4a. Splitting long documents\n",
    "- long documents need to be split into smaller passages so that the inputs can fit into BERT which has a max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4abad753",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384 # this refers to max number of TOKENS - not characters\n",
    "doc_stride = 128 # this is number of overlap, so we do not split long documents inside an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "736015d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens  396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answers': {'answer_start': [30], 'text': ['over 1,600']},\n",
       " 'context': \"The men's basketball team has over 1,600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 NCAA tournaments. Former player Austin Carr holds the record for most points scored in a single game of the tournament with 61. Although the team has never won the NCAA Tournament, they were named by the Helms Athletic Foundation as national champions twice. The team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending UCLA's record 88-game winning streak in 1974. The team has beaten an additional eight number-one teams, and those nine wins rank second, to UCLA's 10, all-time in wins against the top team. The team plays in newly renovated Purcell Pavilion (within the Edmund P. Joyce Center), which reopened for the beginning of the 2009–2010 season. The team is coached by Mike Brey, who, as of the 2014–15 season, his fifteenth at Notre Dame, has achieved a 332-165 record. In 2009 they were invited to the NIT, where they advanced to the semifinals but were beaten by Penn State who went on and beat Baylor in the championship. The 2010–11 team concluded its regular season ranked number seven in the country, with a record of 25–5, Brey's fifth straight 20-win season, and a second-place finish in the Big East. During the 2014-15 season, the team went 32-6 and won the ACC conference tournament, later advancing to the Elite 8, where the Fighting Irish lost on a missed buzzer-beater against then undefeated Kentucky. Led by NBA draft picks Jerian Grant and Pat Connaughton, the Fighting Irish beat the eventual national champion Duke Blue Devils twice during the season. The 32 wins were the most by the Fighting Irish team since 1908-09.\",\n",
       " 'id': '5733caf74776f4190066124c',\n",
       " 'question': \"How many wins does the Notre Dame men's basketball team have?\",\n",
       " 'title': 'University_of_Notre_Dame'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following is an exmaple of a long document that need to be split into smaller documents\n",
    "for i, example in enumerate(datasets[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        print(\"Number of tokens \",len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]))\n",
    "        break\n",
    "example = datasets[\"train\"][i]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6998eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens:  384\n",
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla's record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla's 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 – 2010 season. the team is coached by mike brey, who, as of the 2014 – 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]\n"
     ]
    }
   ],
   "source": [
    "# specifying truncation=\"only_second\", notice the phrase \"the most by the Fighting Irish team since 1908-09.\" was removed\n",
    "truncated_example = tokenizer(example[\"question\"], example[\"context\"], max_length=max_length, truncation=\"only_second\")[\"input_ids\"]\n",
    "print(\"Number of tokens: \", len(truncated_example))\n",
    "print(tokenizer.decode(truncated_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a632dd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67b65918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  384\n",
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla's record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla's 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 – 2010 season. the team is coached by mike brey, who, as of the 2014 – 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]\n",
      "\n",
      "Length:  157\n",
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]\n",
      "\n",
      "Length of overlap:  130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this ong document was split into two shorter documents with number of tokens less than \"max_length\"\n",
    "# notice the length of overlap is indeed \"doc_stride\"/\"stride\"\n",
    "for ids in tokenized_example['input_ids']:\n",
    "    print(\"Length: \",len(ids))\n",
    "    print(tokenizer.decode(ids))\n",
    "    print()\n",
    "overlap = \"championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were\"\n",
    "print(\"Length of overlap: \", len(tokenizer(overlap)['input_ids']))\n",
    "overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd8caf",
   "metadata": {},
   "source": [
    "## 4b. prepare_train_features\n",
    "- This function splits long documents while ensuring that the answer is still intact and uncorrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78b373fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "127020e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ['Saint Bernadette Soubirous']\n",
      "start_positions: [130]\n",
      "end_positions: [137]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] to whom did the virgin mary allegedly appear in 1858 in lourdes france? [SEP] architecturally, the school has a catholic character. atop the main building\\'s gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in using the function, notice the answer still remains in context, which is what we needed\n",
    "# now we have\n",
    "# 1. tokenized ids\n",
    "# 2. start position\n",
    "# 3. end position\n",
    "print(\"Answer:\",datasets['train'][0:1]['answers'][0]['text'])\n",
    "tokenized_example = prepare_train_features(datasets['train'][0:1])\n",
    "\n",
    "print(\"start_positions:\",tokenized_example[\"start_positions\"])\n",
    "print(\"end_positions:\",tokenized_example[\"end_positions\"])\n",
    "\n",
    "\n",
    "tokenizer.decode(tokenized_example['input_ids'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be12950b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ['over 1,600']\n",
      "start_positions: [130]\n",
      "end_positions: [137]\n",
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla's record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla's 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 – 2010 season. the team is coached by mike brey, who, as of the 2014 – 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]\n",
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# notice one long document can produce more than 1 sample containing the answer\n",
    "# so we have more training samples after using the sample\n",
    "i = 249 \n",
    "print(\"Answer:\",datasets['train'][i:i+1]['answers'][0]['text'])\n",
    "print(\"start_positions:\",tokenized_example[\"start_positions\"])\n",
    "print(\"end_positions:\",tokenized_example[\"end_positions\"])\n",
    "\n",
    "tokenized_example = prepare_train_features(datasets['train'][i:i+1])\n",
    "\n",
    "print(tokenizer.decode(tokenized_example['input_ids'][0]))\n",
    "print(tokenizer.decode(tokenized_example['input_ids'][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a55d05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\tanch\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41\\cache-8e21f5a34da7220b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\tanch\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41\\cache-2d9c358a11c9b795.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'end_positions', 'input_ids', 'start_positions'],\n",
       "        num_rows: 88524\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['attention_mask', 'end_positions', 'input_ids', 'start_positions'],\n",
       "        num_rows: 10784\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more samples have been produced due the the splitting function\n",
    "# transformers uses smart caching - the following code needs to be run only once as subsequent runs uses cached data\n",
    "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd8c586",
   "metadata": {},
   "source": [
    "- using the function prepare_train_features() we have prepared our QA pairs into correct input format expected from BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20d38e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>end_positions</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>start_positions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>33</td>\n",
       "      <td>[101, 2073, 2001, 1996, 5871, 9841, 5361, 1029, 102, 1037, 5871, 2598, 2276, 2007, 1037, 1021, 1012, 1020, 1011, 7924, 1006, 2423, 3027, 1007, 5871, 9841, 5361, 1999, 2960, 2012, 1996, 7987, 2401, 2869, 2003, 1996, 2069, 2248, 4434, 4346, 5871, 6971, 2083, 13420, 16846, 3963, 2581, 2000, 18071, 2479, 1998, 1996, 2142, 2983, 1012, 2144, 2035, 2248, 7026, 1998, 4274, 4806, 2024, 18345, 2006, 2023, 2309, 5871, 4957, 2119, 4274, 1998, 7026, 2326, 2024, 3395, 2000, 3103, 2041, 13923, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "      <td>86</td>\n",
       "      <td>[101, 2040, 4520, 3265, 3014, 5918, 2012, 7855, 1029, 102, 1996, 2176, 1011, 2095, 1010, 2440, 1011, 2051, 8324, 2565, 8681, 1996, 3484, 1997, 10316, 2015, 2012, 1996, 2118, 1998, 20618, 7899, 1999, 1996, 2840, 1998, 4163, 1010, 4606, 1996, 22797, 1997, 3330, 1010, 8083, 1010, 4807, 1010, 2189, 1010, 1998, 2495, 1012, 2348, 1037, 3192, 1999, 1996, 4314, 2840, 1998, 4163, 2003, 3223, 1999, 2035, 15279, 1010, 2045, 2003, 2053, 3223, 2691, 4563, 8882, 1025, 3265, 3014, 5918, 2024, 2275, 2011, 1996, 4513, 1997, 2169, 2082, 1012, 7855, 1005, 1055, 2440, 1011, 2051, 8324, 1998, 4619, 3454, 5452, 2006, ...]</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "      <td>96</td>\n",
       "      <td>[101, 2129, 2001, 1996, 2607, 1997, 8387, 2628, 1029, 102, 2045, 2003, 2788, 2019, 12407, 2005, 1037, 3563, 8720, 1997, 2019, 16514, 4005, 2069, 2043, 2107, 8720, 2064, 4681, 1999, 1996, 3949, 2030, 9740, 1997, 1996, 4295, 1010, 2030, 2000, 5083, 3716, 1997, 1996, 2607, 1997, 2019, 7355, 3188, 2000, 1996, 2458, 1997, 4621, 17261, 2030, 4652, 8082, 5761, 1012, 2005, 2742, 1010, 1999, 1996, 2220, 3865, 1010, 3188, 2000, 1996, 3311, 1997, 17207, 2102, 2005, 1996, 3949, 1997, 8387, 1010, 1996, 2607, 1997, 1996, 4295, 2001, 4876, 2628, 2011, 8822, 1996, 5512, 1997, 5776, 2668, 8168, 1010, 2130, 2295, ...]</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the resulting output after applying prepare_train_features()\n",
    "# most important features are\n",
    "# 1. tokenized ids\n",
    "# 2. start position\n",
    "# 3. end position\n",
    "show_random_elements(tokenized_datasets[\"train\"],3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
